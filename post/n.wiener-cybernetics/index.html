<!DOCTYPE html>
<html lang="">

<head>
  <meta charset="utf-8">
  <meta name="generator" content="Hugo 0.82.0" />

  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="author" content="Alexandre Hajjar">
  <meta property="og:url" content="https://www.16h30s.com/post/n.wiener-cybernetics/">

  <title>Synthesis and comments on Cybernetics by N. Wiener - no notation notes</title>
  <meta property="og:title" content="Synthesis and comments on Cybernetics by N. Wiener - no notation notes">
  <meta property="og:type" content="article">
  <meta name="description" content="">

  <link rel="stylesheet" href="/css/fonts.css">
  <link rel="stylesheet" href="/css/highlight.css">
  <link rel="stylesheet" href="/css/journal.css">
  <link href="/index.xml" rel="alternate" type="application/rss+xml" title="no notation notes">

</head>

<body>
  <div class="container">

    <nav class="site-nav">
      <a href="/">Index</a>
    </nav>


  <article class="post">
    <header class="post-header">
      <h1 class="post-title">Synthesis and comments on Cybernetics by N. Wiener</h1>
      <time class="post-date" datetime="2021-02-26 00:00:00 UTC">26 Feb 2021</time>
    </header>

    <h1 id="compte-rendu-par-chapitre">Compte-rendu par chapitre</h1>
<h2 id="i-newtonian-and-bergsonian-time">I. Newtonian and Bergsonian Time</h2>
<p>p32. Newtonian time is defined as the time variable of the physical laws, defined as a <em>&ldquo;formal set
of postulates and a closed mechanics&rdquo;</em>.</p>
<p>üí° The transformation of the variable into its negative has no effect on the mechanics. I believe
it&rsquo;s important to notice that this holds formally pretty obviously (after all, any purely formal
theory will have the characteristics decided by its &ldquo;designer&rdquo;). It also still holds in the real
world (when applying the theory) because of the fact we only consider phenomena from mechanics of
<a href="https://en.wikipedia.org/wiki/Closed_system#In_classical_mechanics">closed systems</a>. But, as we know, the 2nd law of thermodynamics will tell us that any
non-isolated system will see its entropy increase, which wouldn&rsquo;t allow a Newtonian time to be used
by the scientist studying this system.</p>
<p>%causality</p>
<p>p33. He takes a step back:</p>
<blockquote>
<p>Using the Newtonian laws, or any other system of causal laws whatever, all that we can predict at
any future time is a probability distribution of the constrants of the system‚Ä¶</p>
</blockquote>
<p>p34. He asks the question of <em>&ldquo;Why the unidrectional thermodynamics which is based on experimental
terristrial observations stand us in such good stead in astrophysics?&quot;</em>. The explanation is said to
be &ldquo;not too obvious&rdquo;, and I haven&rsquo;t found it clear enough to be convinced. It goes along these
lines: if we imagine a star that is doing the inverse of radiating light to us, we either couldn&rsquo;t
see it, either we would need to be doing our measurement process in time-reverse, which is
impossible. Concluding: <em>&ldquo;Thus the part of the universe which we see must have its past-future
relations, as far as the emission of radioation is concerned, concordant with our own&rdquo;</em>.</p>
<p>p34. More, he asks the question of an <em>&ldquo;intelligent being whose time should run the other way to our
own&rdquo;</em>. This could happen, and we would see phenomena which would be strange (fortuitous,
catastrophic) but still <em>perfectly <strong>explainable</strong></em>. This person would think the same of us, and as
a conclusion, we would be incapable of communicating with each other:</p>
<blockquote>
<p><em>Within any world with which we can communicate, the direction of time is uniform.</em></p>
</blockquote>
<p>This sounds sound, but I am neither very convinced by his demonstration. Let&rsquo;s note that this
precise conclusion is an important cornerstone of any further development in his book.</p>
<p>p35. He makes the link between the theory of evolution (Darwin) and theories based on statistics
(like tidal waves). The overall idea is that real-world dynamical process (let&rsquo;s say: non-closed?)
see <em>&ldquo;fortuitous variability&rdquo;</em> (similar to Darwin&rsquo;s mutations), converted into <em>&ldquo;patterns of
development which [read] in one direction&rdquo;</em>.</p>
<p>p37. He further mentions that Plank and Bohr with their quantum theory have shown that statistical
averaging happens even in the present (not only average through time, but through quantum
probabilties). And thus, even Newtonian physics is <em>&ldquo;the average results of a statistical situation,
and hence an account of an evolutionary process&rdquo;</em>.</p>
<p>üí°‚ùì On the &ldquo;evolutionary process&rdquo; part, it seems he means something close to: time-bound evolution
selects the best possible future (for a species to survive or for a wave to dissipate as much energy
as possible?), but actually quantum-bound evolution selects the best possible present: see quote
below. This sense is very appealing but would definitely need some further explanation and
demonstration.</p>
<blockquote>
<p>the complete collection of data for the present and the past is not sufficient to predict the
future more than staistically</p>
</blockquote>
<p>p38. Phagocytes the Bergsonian dichotomy between the reversible time of physics and the irreversible
time of evolution, mentioning notably that <em>the chance of the quantum theorician is not the ethical
freedom of the Augustinian</em>.</p>
<p>üí°üî¥ This sounds very much a determinist or mechanist argument, going in the metaphysical direction
that he seems to follow throughout the book. He allows himself to say this following the assertion
that he made earlier that the 2nd law of thermodynamics is a direct consequence of the statistical
nature of statistical processes (notably friction forces, etc.).  <strong>This would definitely need to be
further validated, notably in view of how scientists have until now managed to validate the 2nd law
of thermo.</strong></p>
<p>p38. He then mentions that the real first industrial revolution was actually Huyghens and Newton, the
clockwork and engineering of navigation. The new revolution is the <strong><em>&ldquo;age of communication and
control&rdquo;</em></strong>. Notably its main characteristic (making a difference with previous industrial
revolutions): accurate reproduction of signal.</p>
<p>p40. He mentions the Occasionalists, Spinoza and then Leibniz who really found a way to describe
the relation of mind and matter in a dynamical way. The monad is the intellectual device that allows
to think of the world as a continuum of <em>&ldquo;Newtonian solar system[s] writ small&rdquo;</em>, from matter to
mind.</p>
<p>p42. Mentions that wasting energy is a parallel of sending signal (electronic tube). Starts to make
the link with the human body, mentioning automata, that need to be coupled with the outside world
(through messages, information processing). What matters here, according to him, is the
communication engineering: message, noise, information, coding‚Ä¶ He thus sees the 20th century as
<em>&ldquo;the age of servomechanisms as the nineteenth century was the age of the steam engine of the
eighteenth century the age of the clock&rdquo;</em> (hasn&rsquo;t he forgotten electricity?).</p>
<p>üí° Here we see an instance of an intellectual and scientific framewwork that he puts forward at
other points in the book:</p>
<ul>
<li>Make use of analogies, and especially analogies between the (a) natural phenomena that come into
play for the construction of a scientific theory, (b) the devices built by man to test and apply
these theories, (c) the humain body+mind about which functioning we would like to be able to reach
conclusions.</li>
<li>Building synthetic systems that look like versions of the living enables to think &ldquo;back&rdquo; about the
living and take scientific conclusions about how the actual living organisms work. Notably: the
human mind.</li>
</ul>
<p>üí°ü§î We could say this is more of a framework than a tool, and maybe even a political stance, a
program for mankind. This looks definitely like a progress-oriented stance, that adds up on the
Industrial Revolution mindset to give mankind more and more control over nature (itself building
upon the oldest ideologies about technique itself). The real philosophical change about this program
is a new, important, addon: understand ourselves through science and technique.</p>
<p>%arrow-of-time
%explainability
%scientific-method</p>
<h2 id="ii-groups-and-statistical-mechanics">II. Groups and Statistical Mechanics</h2>
<p>p45. Key idea of Gibbs: practice ‚â† Newtonian dynamics, in that we can&rsquo;t know all the initial
velocities and momenta.</p>
<p>p46. To reach the right mathematical conclusions, Gibbs needed to work on inifinite sums of
probabilites 0 to build probabilities &gt;1 and distributions adding to 1. This needed Lebesgue work on
functions as sum of series.</p>
<p>p47. <em>Phase space</em>: N degrees of freedom =&gt; N (position coordinates) + N (momenta) dimensions. Then,
mentions <em>invariants</em> which allow reducing dimensions.</p>
<p>p49. Gibbs had the idea that <strong>time averages and space averages are the same</strong>, this is the <em>ergodic
hypothesis</em>.</p>
<blockquote>
<p>Gibbs himself thought that in a system from which all the invariants had been removed as extra
coordinates almost all paths of points in phase space passed through all coordinates in such a
space. This hypothesis he called the <em>ergodic hypothesis</em>.</p>
</blockquote>
<p>Actually <em>quasi-ergodic hypothesis</em>: system passes indefinitely near any point in the phase space
(limited by invariants).</p>
<p>p50. General considerations about science:</p>
<ul>
<li>&ldquo;For the existance of any science, it is necessary that there exists phenomena which do not stand
isolated.&rdquo;</li>
<li>&ldquo;The essence of an effective rule for a game or a useful law of physics is that it be statable in
advance, and that it apply to more than one case.&rdquo;</li>
</ul>
<p>Then builds up by talking about <em>tranformation groups</em>, <em>group invariants</em>, <em>group character
functions</em> (stable by inversion and multiplication, character functions verifying <code>f(Tx) = a(T)f(x)</code>
with <code>|a| = 1</code>), characterizing character group structure based on the original group structure.</p>
<p>p54. <em>Ergodic theory</em>: theory of metrical invariants of a group of tranformations. He
shows it allows to constructs a justification of Gibbs <strong>interchange of phase averages and time
averages</strong>. His construction is based looking at a transformation T that leaves invariant no set of
points of measure ‚â† 0 or 1, and the limit of character functions which avearge all ∆í(T^n(x)) with
T^n moving in phase or time. These limit functions are shown constant and take the same value, which
is the integral of ∆í (from 0 to 1).</p>
<p>p55. When transformations not ergodic, they can still be reduced to <em>ergodic components</em>.</p>
<p>p55. <em>Entropy</em>, taken from Thermodynamics <code>= ln(probability(region in phase space))</code>.<br>
Thermal equilibrium (max entropy for temporal and volume) =&gt; we can talk about <em>local temperatures</em>.
This works for <em>thermal engines</em> but not <em>living matter</em> where temperature is statistical.</p>
<p>p56. <em>Maxwell demon</em>: the demon is coupled to the box system, <strong>it needs information, which is
negative entropy</strong>. Coupling is a coupling on energy, because of quantum mechanics. After some time,
the demon has random motion (because of temperature) and loses correct perception.</p>
<p>üí° Concise and strong explanation about the Maxwell demon. Interesting to note the link between
energy and information through the fact that <strong>correct perception is reduced by temperature</strong>.</p>
<p>In the same way <strong>enzymes &amp; man are metastable</strong>. Real equilibrium is death.</p>
<blockquote>
<p>The stable state of a living organism is to be dead.</p>
</blockquote>
<h2 id="iii-time-series-information-and-communication">III. Time Series, Information, and Communication</h2>
<p>p60. Contextualization: the time series and their processing needed in case of some systems
(telephone devices, gun pointers‚Ä¶) require automatic chain of operations, able to work on
information itself.</p>
<p>p61. What is information? It is a <em>decision</em>, like the one of choosing Head or Tails.<br>
In context of a quantity known to lie between 0 and 1, the <em>perfectly precise measurement</em> of it would
be represented by an <em>infinite binary number</em> <code>a = 0.a1 a2 a3 a4‚Ä¶</code>. This makes one decision per
binary digit, thus an infinite amount of decisions. <strong>Perfect precision corresponds to an infinite
amount of information</strong>.</p>
<p>In real life, we have a measurement error <code>‚àÜ = 0. b1 b2 b3‚Ä¶</code>, where <code>bk</code> is the first non-0 digit.
With such an error, the first digints of <code>a</code> until <code>a[k-1]</code> and maybe <code>ak</code> are <em>significant</em>, while
the others <code>a[n&gt;k]</code> are <em>not significant</em>. In this context, the number of decisions that are really
taken in the representation of <code>a</code> amount to <code>-log2(‚àÜ)</code> or <code>‚âà k</code> (depending on the significance of
<code>ak</code>). <strong>The amount of information is defined as <code>= -log2(‚àÜ)</code></strong>.</p>
<p>Let&rsquo;s consider that we know <em>a priori</em> that <code>a</code> lies in <code>[0, 1]</code> and that <em>a posteriori</em> (depending
on <code>‚àÜ</code>) it lies in <code>[x, y]</code>. It is also useful to consider the amount of information as <em>how much
smaller</em> the the measure of the interval <code>[x, y]</code> is compared to the measure of <code>[0, 1]</code>. The amount
of information we have from our <em>a posteriori</em> knowledge is <code>log2(|[a, b]|/|[0, 1]|)</code> (<code>||</code> being a
measure).</p>
<p>üí° We see here that <strong>information ‚Üó with precision ‚Üó</strong> and with <code>|[a, b]|</code> ‚Üò (here we talk of
precision of measure).</p>
<p>p62. A more generic version of the above: with a <em>probability</em> that a varable is in <code>[x, x + dx]</code>
defined by <code>f1(x)dx</code>.</p>
<p>üí° We see that <strong><code>f1</code> is a precision</strong>.: where f1 is high, there is less place for the variable to
be defined elswhere in the ‚Ñù line.<br>
Thus, <code>log2(f1)</code> is a good base for a measure of information.  With <code>log2(f1)f1dx</code> we have
normalized this using probability.</p>
<p>Thus <strong><code>‚à´log2(f1)f1dx</code> is total information</strong> (note: <code>‚à´f1 = 1</code>)</p>
<p>p63. Trying to reach conclusions based on these definitions, through calculus.</p>
<blockquote>
<p>The amount of information from independent sources is additive.</p>
</blockquote>
<p>Determining the information gained by fixing one or more variables in a problem.<br>
A priori distrib: <code>u(x)dx = 1/‚àö2œÄa exp(-xÀÜ2/2a) dx</code>.<br>
A posteriori, with <code>u + v = w</code>:</p>
<pre><code>u(x)v(w-x)dx = 1/‚àö2œÄa exp(-xÀÜ2/2a) 1/‚àö2œÄb exp(-(w-x)ÀÜ2/2b) dx
</code></pre><p><em>Excess information</em> is <code>- old apriori + new aposteriori</code>:</p>
<pre><code>excess information = - ‚à´ulog2(u)du + ‚à´u(x)v(w-x)log2u(x)v(w-x))dx
</code></pre><p>We reach conclusions on the message versus noise question:</p>
<blockquote>
<p><strong>the information carried by a precise message in the absence of a noise is infinite.</strong>
In the presence of a noise, however, this amount of information is finite, and it approaches 0
very rapidly as the noise increases in quantity.</p>
</blockquote>
<p>p64. Relationship with thermodynamics, and specifically the 2nd law:</p>
<blockquote>
<p>processes which lose information are, as we should expect, closely analogous to the processess
which gain entropy.</p>
</blockquote>
<blockquote>
<p>No operation on a message can gain information on the average.</p>
</blockquote>
<p>p66. Observation and messages</p>
<blockquote>
<p>A set of observations depends in a arbitrary way on a set of messages and noises with a known
combined distribution. We wish to ascertain how much information these observations give
concerning the messages alone.</p>
</blockquote>
<p>p67. Birkhoff ergodic theorem applied to <em>time series in statistical equilibrium</em> &amp; information of
<em>statistical parameters</em>:</p>
<p>Time recording of time series
&lt;=&gt;
Phase space compute set of stat params of ensemble in stat equilibrium to which time series belong</p>
<blockquote>
<p>given the entire history up to the present of a time series known to belong to an ensemble in
statistical equilibrium, we can compute with probable error zero the entire set of statistical
parameters of an ensemble in statistical equilibrium to which that time series belongs.</p>
</blockquote>
<p><code>(an)</code> Fourier params calculated from time <code>‚à´[e^t t^n f(t)dt]t&lt;0</code>
=&gt;
Knowledge (distribution) of <code>A(t)</code> defined for <code>t&gt;0</code> (in the future).</p>
<p>We can obtain the <em>prediction</em> to meet any desired criterion of <em>goodness</em>.</p>
<p>Knowledge of the past
=&gt;
Compute amount of information we have of the future beyond a certain point <code>t1 &gt; 0</code>.</p>
<p>p70. Study of <em>Brownian motion</em> based systems and their prediction.</p>
<p><em>Brownian motion</em>: mean square motion in direction proportional to length of time, motions
time-uncorrelated.</p>
<pre><code>0    -&gt; t1: ‚àÜx1
tn-1 -&gt; tn: ‚àÜxn
</code></pre><p>Probability that particles lie in <code>[x1, x1+dx]</code> at t1 &amp; ‚Ä¶ &amp; <code>[xn, xn + dx]</code> at tn</p>
<pre><code>= exp[-x1^2/2t1 - (x2 - x1)^2/2(t2 - t1)‚Ä¶]/‚àö[(2œÄ)^n t1(t2 - t1)‚Ä¶] dx1dx2‚Ä¶
</code></pre><p>Set of paths correponding to the ‚â† possible Brownian motions can depend on param <code>Œ± ‚àà [0, 1]</code> so
that each path is a function of <code>x(t, Œ±)</code> and where the &ldquo;probability that a path lies in a certain
set S is the same as the measure of the set of values of Œ± corresponding to paths in S&rdquo;.</p>
<p>He proves:</p>
<pre><code>‚à´0-&gt;1[x(t1, Œ±)x(t2, Œ±)‚Ä¶x(tn, Œ±)]dŒ± = Œ£Œ†‚à´0-&gt;1[x(tj, Œ±)x(tk, Œ±)]dŒ±
</code></pre><blockquote>
<p>when we know the averages of the products of x(tj, Œ±) by pairs, we know the averages of all
polynomials in these quantities, and thus their entire statistical distribution.</p>
</blockquote>
<p>p72. We look at <code>‚à´K(t)dŒæ(t, Œ≥)</code>, <code>K</code> any function with <code>Œæ</code> running <code>t</code> on ‚Ñù with</p>
<pre><code>Œæ(t, Œ≥) = Œæ(t, Œ±, Œ≤)
Œæ(t, Œ±, Œ≤) = x(t, Œ±) if t &gt;= 0
Œæ(t, Œ±, Œ≤) = x(-t, Œ≤) if t &lt; 0
</code></pre><p><code>Œ≥</code> being a mapped from ‚Ñù¬≤ onto ‚Ñù (via diagonalisation).</p>
<p>He proves that, of time series <code>f(t, Œ≥) = ‚à´K(t + œÑ)dŒæ(t, Œ≥)</code>, all statistical parameters depend on
the <em>autocorrelation function with lag</em> <code>œÑ</code> of <code>K</code>, and thus it is <strong>in statistical equilibrium</strong>.</p>
<p><em>Autocorrelation function</em>: <code>Œ¶(œÑ) = ‚à´K(s)K(s + œÑ)ds</code></p>
<p>We can thus show that for any bounded measurable <em>functional</em> (quantity depending on the netire
distribution of the values of the function of <code>t</code>), we have</p>
<pre><code>lim œÉ-&gt;‚àû ‚à´0-&gt;1[‚Ñ±(f(t, Œ≥))‚Ñ±(f(t + œÉ, Œ≥))]dŒ≥ = {‚à´0-&gt;1[‚Ñ±(f(t, Œ≥))]dŒ≥}¬≤
</code></pre><p>and by <em>ergodic theorem</em>:</p>
<pre><code>‚à´0-&gt;1[‚Ñ±(f(t, Œ≥))]dŒ≥ = lim T-&gt;‚àû 1/T ‚à´-T-&gt;0[‚Ñ±(f(t, Œ≥))]dt
</code></pre><p>Thus</p>
<blockquote>
<p>we can almost always read off any statistical parameter of such a time series, and indeed any
denumerable set of statistical parameters, from the past history of a single example.</p>
</blockquote>
<p>p76. &ldquo;Try to build up a time series as general as possible from the simple Brownian motion series.&rdquo;</p>
<p>Time series in the form:</p>
<pre><code>‚à´a-&gt;b [exp(i ‚à´K(t + œÑ, Œª)dŒæ(œÑ, Œ≥))] dŒª
</code></pre><p>This is</p>
<blockquote>
<p>a first step toward the solution of the problem of reducing a large class of time series to a
canonical form, and this is most important for the concrete formal application of the theories of
prediction and of the measurement of information</p>
</blockquote>
<p>p78. To further generalize</p>
<blockquote>
<p>the question is:  under what circumstances can we represent a time series of known statistical
parameters as determined by a Brownian motion(‚Ä¶)?]</p>
</blockquote>
<p>He concludes that this is a research program which &ldquo;offers the best hope for a rational, consistent
treatment of many problems associated with non-lineat prediction‚Ä¶&rdquo;.</p>
<p>p80. Going back to the <em>prediction problem</em> for time series <code>‚à´K(t)dŒæ(t, Œ≥)</code>.</p>
<p>He shows that the past and present of the <em>differential</em> <code>dŒæ(t, Œ≥)</code> determines the past and present
of the time series, and conversely.</p>
<p>He then builds <em>best prediction operators</em> in context of <em>message</em> + <em>noise</em>.</p>
<p><em>Wave filters</em> with lag <code>a</code> are used to build the &ldquo;best representation&rdquo; of <code>m(t + a)</code> (written
frequency scale) (<code>m</code> is the message).</p>
<p>p86. <em>Rate of transmission of information</em> in case of messages and noises derived from the Brownian
motion.</p>
<p>He calculates the total amount of information avialable concerning the distribution of the message,
then the <em>rate of transmission of information</em>. It depends not only on the <strong>width of the frequency
band</strong> available for transmitting the message but also on the <strong>noise level</strong>.</p>
<p>p88. Concludes on the &ldquo;theory of messages depending lineraly on the Brownian motion&rdquo;:</p>
<ul>
<li>it gives the <strong><em>best possible design</em> of predictors and wave filters</strong> in case message and noise
represent the response of <strong>linear</strong> resonators to Brownian motions</li>
<li>in much more general case, they represent a <strong>possible design for predictors and filters</strong>.</li>
</ul>
<p>p88. Mentions <em>multiple time series</em>: more complex.</p>
<p>p89. Mentions <em>discrete time-series</em>:</p>
<blockquote>
<p>by a process of step-by-step predicition, we can solve the entire problem of linear prediction
for discrete time series.</p>
</blockquote>
<blockquote>
<p>The filters for discrete time series are usually not so much physically constructible devices to
be used with an electric circuit as mathematical procedures to enable statisticians to obtain the
best results with statistically impure data.</p>
</blockquote>
<p>p92. Caveat: the statistical theories here involve a <em>full knowledge of the pasts</em>. To go further,
we have to extend existing methods of <em>sampling</em>, which most probably requires Bayes' law.</p>
<p>p92. Discussion on quantum mechanics. Progression:</p>
<ul>
<li>Newtonian physics: the sequence of physical phenomena is completely determined by all positions
and momenta at any one moment.</li>
<li>Gibbsian theory: with a perfect determination of the multiple time series of the whole universe
the knowledge of  all positions and momenta at any one moment would determine the entire future.</li>
<li>Heisenberg: &ldquo;time series can in no way be reduced to an assembly of determinate threads of
development in time&rdquo;.</li>
</ul>
<blockquote>
<p>In quantum mechanics, the whole past of an individual system does not determine the future of that
system in any absolute way but merely the distribution of possible futures of the system.</p>
</blockquote>
<p>Classical physics worked &ldquo;<em>over the range of precision where it has been shown experimentally to be
applicable</em>&rdquo;.</p>
<p>Mentions that high wavelength light waves allows high precision measurement of a system but will
subject it to a change in momentum.</p>
<p>p93. Asserts that this theory is still <strong>applicable to the <em>theory of entropy</em></strong>.</p>
<p>A system will transform itself in the course of time into any other state, but the probability of
this depends of the <em>relative probability of measure of the two states</em>. The probability is high for
states which can be transformed into themselves by a large number of transformations, or having
<em>high internal resonance</em>.</p>
<p>High internal resonance allows for more stability.</p>
<p>Haldane: this is maybe how genes and viruses reproduce themselves. He notes that there is no sense in
mentioning which gene is a copy or the master version, as there is no such thing as perfectly sharp
individuality on a quantum level.</p>
<p>Szent-Gy√∂rgyi: substances with high resonance have an &ldquo;abnormal capacity&rdquo; for sotring both energy
and information.</p>
<h2 id="iv-feedback-and-oscillation">IV. Feedback and Oscillation</h2>
<p>p96. <em>Feedback</em>, <em>chain of feedback</em>, <em>negative feedback</em></p>
<blockquote>
<p>Our motion is regularted by some measure of the amount by which it has not yet been accomplished.</p>
</blockquote>
<p>p97. Output of the <em>effector</em>. In this section, he talks about linear effectors, and their problems.</p>
<p>p98. Pieces of apparatus which delay inputs =&gt; <code>∆í(t - œÑ)</code></p>
<p>We can approx <code>‚à´0-&gt;‚àû ak ∆í(t - œÑk) dœÑ</code> (&ldquo;Operator&rdquo;)</p>
<p>üí° A small note on non-determinism in text:</p>
<blockquote>
<p>‚Ä¶ a streetcar which may turn off one way of the other at a switch, which is not determined by its
past.</p>
</blockquote>
<p>The expression above is <strong>independent of a shift of the origin of time</strong> and <strong>linear</strong>. All
such operators of the past have this form, or limit of a sequence of.</p>
<p>p100. Look at <code>∆í(t) = exp(zt)</code></p>
<p>=&gt; Operator is <code>exp(zt) ‚à´0-&gt;‚àû a(œÑ)exp(-zt) dœÑ = exp(zt) A(z)</code></p>
<p><code>A(z)</code> is the <em>representation of the Operator as a function of frequency</em>.</p>
<p>Remarks on the mathematical content:</p>
<pre><code>|A(x + iy)| &lt;= ‚àö[1/2x ‚à´0-&gt;‚àû |a(œÑ)|¬≤ dœÑ ]
</code></pre><p>=&gt; A is bounded in every half-plane <code>x &gt;= Œµ &gt; 0</code> with <code>A(iy)</code> the boundary values.</p>
<p>üí° Indeed, <code>x -&gt;+ 0</code> defines the &ldquo;max boundary&rdquo; <code>‚àÄ x&gt;0</code> as per the expression above, and this is
<code>A(x + iy) -&gt;+ A(iy)</code>.</p>
<p>p101. With <code>A(x + iy) = u + iv</code> and notably <code>x=0</code>, study the looks of the boundary.</p>
<p>üåÖ <strong>See figure page 101</strong>. The idea here is that interior points are reached taking the normal on the
right of the line drawn by <code>A(iy)</code>, without crossing the line again.</p>
<p>üí° The author notes that interior points correspond to possible values of <code>A(x + iy), x&gt;0</code>. Not sure
what is the link between right normals to <code>A(iy)</code> and this.</p>
<p>p102. <em>Control flow chart</em> of such a system.</p>
<p>üåÖ <strong>See figure page 102.</strong></p>
<p><code>Y = X - ŒªAY</code> with X the input, Y the input to the motor, the motor operator is A, and multiplier
operator is Œª.</p>
<p>The motor output is <code>A Y = A/(1 + ŒªA) X</code>, <code>A/(1 + ŒªA)</code> being the operator with a diagram of</p>
<pre><code>u + iv = A(iy)/(1 + ŒªA(iy))
</code></pre><ul>
<li>‚àû is an interior point iff -1/Œª is an interior point of A(iy), which corresponds to unrestrained
and increasing oscillation.</li>
<li>If -1/Œª is an exterior point, the feedback is stable.</li>
<li>If -1/Œª is on the boundary, it&rsquo;s more complicated, but overall there will be an oscillation with
an amplitude which does not increase.</li>
</ul>
<p>p103. The author examines a series of different operators to study their feedback range.</p>
<p>If <code>A(z) = z</code>, <code>A(iy)</code> is the bottom-up directed ordinate line, thus all the right ‚ÑÇ plane is
interior. -1/Œª is always exterior, thus any feedback is ok.</p>
<p>If <code>A(z) = u + iv = 1/(1 + kz)</code>, it&rsquo;s equivalent to <code>u¬≤ + v¬≤ = u</code>: the circle with radius ¬Ω and
center at (¬Ω, 0) described clockwise, thus with interior points interior to the circle. -1/Œª is
always exterior, so any feedback is ok.<br>
Note: corresponding a(t) is <code>a(t) = exp(-t/k)/k</code>.</p>
<p>If <code>A(z) = (1/(1 + kz))¬≤</code>, it&rsquo;s (in polar) <code>‚àöœÅ = - sin œï/2</code> or <code>‚àöœÅ = cos œï/2</code> which is a cardiod
clockwise, this with interior point being interior. All feedback is ok.</p>
<p>If <code>A(z) = (1/(1 + kz))¬≥</code>, it&rsquo;s (in polar) <code>‚àõœÅ = cos œï/3</code>: üåÖ  see figure page 105. -1/Œª is interior
iff <code>Œª &gt; 8</code>.</p>
<p>If <code>A(z) = exp -Tz</code> a delay in time, then <code>u + iv = exp -Tiy = cos Ty - isin Ty</code> a clockwise unit
circle centerd on origin. -1/Œª is interior iff <code>-1/Œª &gt; -1</code> or <code>Œª &gt; 1</code>. The limit of feedback
intensity is thus 1.</p>
<p>As a conclusion, we see that we can compensate for an operator <code>1/(1 + kz)</code> by an arbitrarily heavy
feedback, so as to get <code>A/(1 + ŒªA)</code> as near to 1 (not 1/Œª?) as we wish for all frequencies.<br>
Considering <code>1/(1 + kz)</code> as an operator and <code>(1/(1 + kz))À£</code> as a composition of x of them, we see
that <strong>we can compensate one operator with one feedback</strong>, even two operators with one feedback, but
3 operators will need at least 2 feedbacks.</p>
<p>p106. Ship steering stabilizing system using gyrocompass: this yields a <code>(1/(1 + kz))¬≥</code> kind of
operator, thus &ldquo;no servomechanism whatever will stabilize the system&rdquo;.</p>
<p>We can achieve stabilization by using another feedback which would be, for example, the difference
between the actual course and the angular position of the rudder.</p>
<p>p108. Link with <em>physiological cybernetics</em>:</p>
<blockquote>
<p>One of the great tasks of physiological cybernetics is to disentangle and isolate loci of the
different parts of thie complex of voluntary and postural feedbacks.</p>
</blockquote>
<p>üí° We might appreciate the &ldquo;disentangle and isolate&rdquo; approach, which is a direct application of
rationality by practicing <em>analysis</em>. And this is precisely what is also causing moral problems when
this starts to be not only analysed (disentangled‚Ä¶) but also acted upon.</p>
<p>p108. Effects of heavy feedback</p>
<blockquote>
<p>When feedback is possible and stable, its advantage, as we have already said, is to make
performance less dependent on the load.</p>
</blockquote>
<p>‚ÜóÔ∏è  negative feedback (if stable) =&gt; ‚ÜóÔ∏è  stability of the system <strong>for low frequencies</strong> but ‚ÜòÔ∏è
stability for some <strong>high frequencies</strong>.</p>
<p>p108. Incipient osciallation: corresponds to y with A(iy) on the boundary with u most on the left
(negative).</p>
<p>p109. Conclusion on the previous analysis of linear osciallating systems.</p>
<p><strong>These linear oscillating systems nearly always oscillate in the form <code>A sin (Bt + C) exp Dt</code>.</strong><br>
<strong>If there is a periodic non-sinusoidal oscillation, &ldquo;it is always as suggestion at least that the
variable observed is one in which the system is not linear&rdquo;.</strong></p>
<p>Also, <strong>for linear oscillations, the amplituode of oscillation is independent of frequency.</strong><br>
Whereas <strong>for non-linear osciallations, there is a discrete set of amplitudes for which the system
will oscillate at a given frequency, and only a discrete set of such frequencies.</strong></p>
<p>Example: organ pipe. This can be described by a <em>relaxation oscillation</em>: solution periodic in time
and determinate in amplitude and frequency but not in phase.</p>
<p>Some systems are non-linear but can be studied as linear when non-linear terms are mostly constant
over a period: <em>theory of secularly perturbed systems</em>.</p>
<p>p110. Non-linear systems of relaxation oscillation: well studied when differential equations of low
order. But not well studied at the time: integral equations when system depends for its future
behavior on its entire past behavior. The author sketches a solution with an infinite system of
linear non-homogeneous differential equations.</p>
<p>p111. Competition between <em>feedback systems of control</em> (this chapter) and <em>compensation systems</em>
(previous chapter). <strong>&ldquo;Both serve to bring the complicated input-ouput relations of an effector into
a form approaching a simple proportionality&rdquo;</strong>.</p>
<p>The feedback system has a <strong>performance relatively independent of the characteristic and (changes
of) of the effector used</strong>. One should select the method based on the constancy of the
characteristic of the effector.</p>
<p>Study of the cases where it&rsquo;s advantageous to combine both methods.</p>
<p>üåÖ See fig 4 page 112: there is a Compensator before the Substractor, which can compensate the
average characteristic of the feedback system.</p>
<p>üåÖ See fig 5 page 112: the Compensator is put right after the Substractor (one larger effector with
the Effector). In general, this affects the max feedback, but for the same feedback level will
improve the performance of the system. For example: if Effector has a lagging characteristic, the
Compensator will be a predictor.</p>
<p>p113. Makes the link with human/animal reflexes.</p>
<p>Duck shooting error minimization is <em>anticipatory feedback</em>.</p>
<p>Steering of a car on an icy road: <em>control vy informative feedback</em>.<br>
üåÖ See fig 6 page 114: this one can be modeled with a high-frequency oscillator (briging
information), and a Compensator that explores amplitude-phase relations of the high-frequency output
to the input.<br>
Advantages of this type of feedback: &ldquo;the Compensator may be adjusted to give stability for every
type of constant load&rdquo;. Secular load change: OK (like gun turret friction that goes up very
slowly).<br>
&ldquo;This informative feedback will work well only if the characteristics of the load at high frequencies
are the same as, or give a good indication of, its characteristics at low frequencies.&rdquo;</p>
<p>p114. Link with <em>homeostasis</em>.</p>
<blockquote>
<p>our inner economy must contain an assembly of thermostats, automatic hydrogen-ion-concentration
controls, governors, and the like, which would be adequate for a great chemical plant. These are
what we know collectively as our homeostatic mechanism.</p>
</blockquote>
<p>Homeostatic feedbacks are slower than voluntary and postural feedbacks:</p>
<ul>
<li>nerve fibers: para-/sympathetic systems (non-myelinated, slow) + effectors: smooth muscles and glands (slow)</li>
<li>or non-nervous channels: slower modes of transmission.</li>
</ul>
<h2 id="v-computing-machines-and-the-nervous-system">V. Computing Machines and the Nervous System</h2>
<p>p116. Recording numbers: use a uniform <em>scale</em> (ie base).</p>
<p>Amount of information: <code>I = log‚ÇÇ n</code>,<br>
cost of recording information: <code>(n - 1) A = (2^I - 1) A</code> with A constant.</p>
<p>Divided among N scales: <code>N (2^[I/N] - 1) A</code>.</p>
<p>It is shown that minimum of the cost is reached with <code>N = ‚àû</code>.<br>
To get N as large as possible and keep <code>2^[I/N]</code> an integer, we use <code>I/N = 1</code>.</p>
<p>p117. The <strong><em>binary system</em></strong>:</p>
<blockquote>
<p>in which all that we know is that a certain quantity lies in one of the other of two equal
portions of the scale, and in which the probability of an imperfect knowledge as to which half of
the scale contains the observation is made vanishingly small</p>
</blockquote>
<pre><code>v = v0 + 1/2 v1 + 1/2¬≤ v2 + ‚Ä¶ + 1/2‚Åø vn + ‚Ä¶
</code></pre><p>with vn in {0, 1}.</p>
<p>p117. Different types of machines:</p>
<ul>
<li><em>Analogy machines</em>: data are represented by measurements on some continuous scale.</li>
<li><strong><em>Numerical machines</em></strong>: data are represented by a <strong>set of choices among a number of
contingencies</strong>.  Which will be more accurate.</li>
</ul>
<p>Mentions that in a chain of computations, it is the slowest which gives the order of magnitude of
the entire system. Thus, <strong>remove the human from the chain and perform all intermediate processes
on the binary scale</strong>.</p>
<p>p118. <em>Algorithms</em> for <em>combining contingencies</em>: those are the rules for combining the numerical
data in input.</p>
<p>The most obvious one is <em>Boolean algebra</em>, which is considered superior to other systems on the same
bases than superiority of binary arithmetic.</p>
<p>üí° Of course this is true only when minimizing cost of recording of information, not when
considering for example the epistemological questions of what corresponds most to human experience.</p>
<p><strong>All is just ‚ëÉ</strong>:</p>
<blockquote>
<p>Thus all the data, numerical or logical, put into the machine are in the form of a set of choices
between two alternatives, and all the operations on the data take the form of making a set of new
choices depend on a set of old choices.</p>
</blockquote>
<p>p119. Decribes basically a CPU architecture: <em>bank of relays</em>, with conditions &ldquo;on&rdquo; and &ldquo;off&rdquo;, with
positions dictated by the positions of others at a previous stage. And with a <em>central clock</em>.</p>
<p>Then introduces <em>memory</em>: &ldquo;special apparatus to retain an impulse which is to act at some future
time&rdquo;.</p>
<p>p120. Nerous system: <em>neurons</em> are either firing or reposing, mostly based on input messages from
other neurons through <em>synapses</em> (from few to a 100s).</p>
<p>Explains simplified firing with a clock (delay) and a treshold: if inputs before delay are above
threshold, then the neuron will fire at end of current interval of time.</p>
<p>Memory of the nervous system: &ldquo;preserve results of past operations for use in the future&rdquo;.</p>
<p>p121. Brain vs machine:</p>
<blockquote>
<p>the brain, under normal circumstances, is not the complete analogue of the computing machine but
rather the analogue of a single run on such a machine.</p>
</blockquote>
<p>p122. Memory challenges: &ldquo;difficult to achieve a considerable time lag&rdquo;. For example, use elastic
vibrations. But before the cumulative deformation of the message becomes too big, &ldquo;<em>trigger off a
new message</em> of prescribed form&rdquo;, like in telegraph type repeaters.</p>
<p>Condensers look like a good solution in some cases.</p>
<p>For more <em>permanent records</em>, lists solutions (magnetic tape, phosphorecent substances,
photography).</p>
<p>Notices that methods of storage of information share a physical element in common: they depend on
systems with a high degree of quantum degeneracy, a large number of modes of vibration of the same
frequency.</p>
<blockquote>
<p>Quantum degenreacy appears to be associated with the ability to make small causes produce
appreciable and stable effects.</p>
</blockquote>
<p>Notes that in case of the neural system, long-term memory is a more permanent change like treshold
change through permeability of synapses, ‚Ä¶</p>
<p>Notes that if the chief change of tresholds in the memory process are increases, then &ldquo;the capital
stock of power to live might decrease&rdquo;. This might be the cause of senescence.</p>
<p>üí° Probably not true, to double check.</p>
<p>p124. Look at the light cast on logic by such machines.</p>
<blockquote>
<p>The science of today is operational; that is, it considers every statement as essentially
concerned with possible experiments or observable processes. According to this, the study of
logic must reduce to the study of the logical machine‚Ä¶</p>
</blockquote>
<p>This might reduce logic to psychology. On their difference:</p>
<blockquote>
<p>All logic is limited by the limitations of the human mind when it is engaged in that activity
known as logical thinking.</p>
</blockquote>
<p>For example, &ldquo;No admissible proof involves more than a finite number of stages&rdquo;. He mentions
<em>mathematical induction</em> (which may span an infinite number of cases), which is different from
complete induction over an infinite set (which is impossible).<br>
To prove <code>Pn</code> for all n, we need a single argument independent of n (mathematical induction).<br>
These things are studied in metamathematics, discipline built by G√∂del.</p>
<p>p126. &ldquo;A logical machine following definite rules need never come to a conclusion&rdquo;.</p>
<p>There he makes a parallel with paradoxes (like Cantor or Russel paradoxes), where the answer would
oscillate between &ldquo;yes&rdquo; and &ldquo;no&rdquo;.</p>
<p>Mentions Russel&rsquo;s solution using types, &ldquo;attach a parameter to each statement, ‚Ä¶ being the time at
which it is asserted&rdquo;.</p>
<p>p126. What about the <em>ability to learn</em>? Consider two related notions: <em>association of ideas</em> and
<em>conditioned reflex</em>. Based on Locke &amp; Hume&rsquo;s terms, <em>ideas and impressions</em> unite themselves into
bundles based on <em>similarity</em>, <em>contiguity</em> and <em>cause and effect</em>.</p>
<p>Mentions that <em>dynamics</em>-thinking had not yet reached biology and physiology in the 18th century, it
was rather <em>collection</em>-oriented, because there was &ldquo;so much to explore&rdquo;.
Pavlov&rsquo;s work is a good example of the first steps into dynamics.</p>
<p>p127. <em>Affective tone</em></p>
<p>Pavlov has shown &ldquo;union of patterns of behavior&rdquo; (food shown together with another object to the
dog), similar to Locke&rsquo;s &ldquo;union by contiguity&rdquo;. But Pavlov is looking at visible actions, not
introspective states of mind / ideas like Locke.</p>
<p><em>Affective tone</em> is a central element of conditioned reflex: it is a scale from pain to pleasure.</p>
<blockquote>
<p>an increase in affective tone favors all processes in the nervous system that are under way at the
time</p>
</blockquote>
<blockquote>
<p>even the most suicidal apportioning of affective tone will produce a definite pattern of conduct.</p>
</blockquote>
<p>üåÖ Fig. 7 p.128: this shows parallel processes which produce an affective-tone mechanism, which in
turn feed a &ldquo;totalizer&rdquo; which feeds back into processes.</p>
<p>When affective tone ‚ÜóÔ∏è  , the feedback makes thresholds ‚ÜòÔ∏è  . And inversely.</p>
<p>p129. We thus see that affective tone (only a model) is <strong><em>capable of learning</em></strong>.</p>
<p>Notes that the totalizer may just be a specific messaging system, like hormones which are efficient
for &ldquo;to whom it may concern&rdquo; kind of messages (I would say publish-subscribe).</p>
<p>This shows that we should take into account hormonal transmission.</p>
<p>He notes, while being not sure of the scientific validity of this, that Freud&rsquo;s theories put sex and
memory very close to each other. Which doesn&rsquo;t seem &ldquo;absurd in principle&rdquo;.</p>
<p>p130. Machines can potentially have conditioned reflexes.</p>
<p><em>Run</em> of mechanical structure of the computing machine === <em>life</em> of the individual.</p>
<p>Idea for an artifical neuro-hormonal learning system: a message can change the grid bias of a number
of vacuum tubes.</p>
<p>p130. More developed use of computing machines: solution of partial differential equations.</p>
<p>In case of non-linear PDEs, there is the idea that numerical calculations are going to <strong>create the
data needed to start building a theory</strong>, instead of looking for them in nature:</p>
<blockquote>
<p>as von Neumann has pointed out, we need them in order to form that acquaintance with a large
number of particular cases without which we can scarcely formulate a general theory.</p>
</blockquote>
<p>üí° It&rsquo;s fun to see that we are talking AI some pages before, and here about the fact that numerical
calculations are feeding theory modeling. If you make these two ideas a bit closer: on the AI end,
this makes reinforced learning, and on the theory end, this makes‚Ä¶ artificial worlds science?</p>
<p>p131. Sketches an even more precise computer architecture:</p>
<ul>
<li>Make common oprations (add, multiply) as &ldquo;standard assemblages&rdquo; whereas less frequent would use
these.</li>
<li>Component parts should be generic, not tied to other specific apparatus.</li>
<li>Allot components as needed through a kind of networking switch.</li>
</ul>
<p>Makes the link with &ldquo;traffic problems and overloading in the nervous system&rdquo;.</p>
<p>p132.</p>
<blockquote>
<p>The mechanical brain does not secrete thought &ldquo;as the liver does bile&rdquo;, as the earlier
materialists claimed, ‚Ä¶ Information is information, not matter or energy.</p>
</blockquote>
<h2 id="vi-gestalt-and-universals">VI. Gestalt and Universals</h2>
<p>p133. Locke&rsquo;s theory of <em>association of ideas</em>: three principles:</p>
<ul>
<li><strong>Contiguity</strong>.</li>
<li><strong>Similarity</strong>.</li>
<li>Cause &amp; effect, reduced by Locke and Hume to contiguity.</li>
</ul>
<p>The author makes the hypothesis that, based on the previous chapter, there are neural mechanisms
corresponding to these.</p>
<p>p133. Focus on <em>similarity</em>, in case of recognizing someone.</p>
<p>This involves a <em>visual-muscular feedback system</em>, which is abundant in animals, and can be largely
superseded in terms of speed by an artifical mechanism thanks to electrical techniques.</p>
<p>2 eye-muscles feedbacks in man:</p>
<ul>
<li>Of <em>homeostatic</em> nature, like pupil opening in the dark.</li>
<li>Of <em>reflex</em> nature, like putting an interesting object (moving, brilliant) in the fovea which is
better for form and color.</li>
</ul>
<p>For the 2nd case, the goal to make the image of the object vary &ldquo;within as small a range as
possible&rdquo;. This is kind of an optimisation that &ldquo;<strong>diminishes the number of neuron channels
involved</strong> in the transmission of visual information&rdquo;.</p>
<p>p135. The author discusses what we would call now feature specialization of parts of the neural
process</p>
<p>Recognition of outline drawing: this seems to be explained by the fact that &ldquo;somewhere in the visual
process, outlines are emphasized&rdquo; compared to other &ldquo;aspects of an image&rdquo;.</p>
<p>üí° These aspects are now called <em>features</em> in a ML model.</p>
<p>Retina is subjet to <em>accomodation</em>: contant stimulus =&gt; transmission of it ‚ÜòÔ∏è  .<br>
This allows not changing the character of an image which is stared at.</p>
<p>Mentions photography plaate treatments that increase contrasts, and thus allow <em>repeating</em> like in
telegraph-type repeaters: trigger a new impression of standard sharpness from an image not too much
blurred.</p>
<p>p136. The structure of the visual cortex is not using a highly generalized mechanism but corresponds
to a <em>permanent sub-assemly</em> of specialized parts.</p>
<p>Consider the <em>perspective transformations</em> of an object, as a group (in math sense). Some of them
(rotation, translations‚Ä¶) have continuously varying parameters. They &ldquo;form multi-dimensional
configurations in n-space, an contain sub-sets of transformations which constitute regions in such a
space&rdquo;.</p>
<p><em>Group scanning</em> allows to traverse a net of positions in a one-dimensional sequence. We can also
approximate any transformation as near as we wish through transformations of this sequence, as long
as scanning is fine enough.<br>
We consider <em>regions of maximum dimensionality</em> of regions transformed by the group under
consideration.</p>
<p>Descibes a mechanizable process to identify the shape of a figure: scan and compare tranformation
regions result with a fixed pattern and mark regions alike if these coincide (not clear here if it&rsquo;s
about comparing the result of transformations with the &ldquo;front view&rdquo; of an object or something along
these lines).</p>
<p>p138. Modelisation of <em>Gestats</em>.</p>
<p>Notices that <em>seems like</em> relation, as described by the previous process, is not necessarily
transitive: if A === B and B === C, we can still have A !== C.</p>
<blockquote>
<p>The universal &ldquo;ideas&rdquo; thus formed are not perfectly distinct but shade into one another.</p>
</blockquote>
<p>üí° I would like to say &ldquo;computable&rdquo; for anything that relates a mechanizable process.</p>
<p>The author introduces a process that allows to calculate some quantity <code>Q</code> for a set <code>S</code> of
elements, tranformed by transformations <code>T</code>. If we integrate these quantities of the group measure
(done with group scanning based on probability density of transformations), like <code>‚à´ Q(TS) dT</code>, we
have a quantity that &ldquo;<strong>will be identical</strong> for all sets S interchangeable with one another under
the trasnformations of the group, that is, <strong>for all sets S which have in some sense the same form
or <em>Gestalt</em></strong>&rdquo;.</p>
<p>p139. Notices that sound-enabled reading for blind (&quot;<em>prothesis</em> of one lost sense by another&rdquo;) is
possible based on the &ldquo;<em>visual Gestalt</em>&rdquo; processes like page alignment, traversing from one line to
anohter‚Ä¶</p>
<p>Produces a tentative sound encoding of letters, a bit like Braille.</p>
<p>p140. <em>Group scanning assembly</em>.</p>
<p>Notices the problem of scanning letters by photocells, notably in terms of height.  This needs
transformation of the vertical dilation group.</p>
<p>üåÖ  Fig 8 describes such a mechanism by McCulloch: photocells, leads, oscillators and connections.</p>
<p>IT represents a type of device &ldquo;usable for any sort of group scanning&rdquo;.<br>
It is suggested that the fourth layer of the visual cortex might work (or at least be modeled) that
way.<br>
Also, ear transposition of fundamental pitch to the other is a translation of logarithm of
frequency: might be perfomed by such a device as well.</p>
<blockquote>
<p>the <em>group-scanning assembly</em> is well adapted to form the sort of permanent sub-assembly of the
brain‚Ä¶</p>
</blockquote>
<p>üí° It&rsquo;s kind of a neural network architecture in which layers are able to organise so as to build
interesting transformations (in the sense they are related to a given goal, training) that isolate
features.</p>
<p>p141. Performance of the scanning apparatus.</p>
<p>The order of magnitude is the time required to make a direct comparison of shapes of objects
different in size.</p>
<p>Seems to be around: 1/10th of a second, which accords well with the time needed to stimulate all the
layers of connectors.</p>
<p>üí° Interesting to see the relationship with medical/physiological experimentation, and the way he
does compare results of such experiments with the model to justify that the model is accurate.</p>
<p>p141. <em>Widespread synchronism</em> and clocking mechanism in the cortext.</p>
<blockquote>
<p>there is a widespread synchronism in different parts of the cortex, suggesting that it is driven
from some clocking center.</p>
</blockquote>
<p>And the frequency is on the same order as the <strong><em>alpha rhythm</em></strong> of the brain.<br>
Alpha rhythm is compared to the scanning rhythm of a television apparatus, but for group scanning.<br>
This rhythm is most marked in meditation, but changes function (carrier) during concentration,
and disappears in sleep‚Ä¶</p>
<p>p142. Concludes that <em>sensory prothesis</em> is possible.</p>
<blockquote>
<p>memory and association areas (‚Ä¶) are available to store impressions gatherd from other senses than
the one to which they normally belong.</p>
</blockquote>
<p>Thus, a blinded man (not congenital blind) keeps a part of his normal visual mechanism. But he loses
the fixed assembly part.<br>
Prothesig this would need not only artificial visual receptors but also an artificial visual cortex.</p>
<p>üí° That&rsquo;s the program for Neuralink basically. We can note the hardware vs software distinction, and
we can even say that this is a strong methodological accomplishment of the author to build an
understandable distinction between what is hardware and what is software.</p>
<p>Compares vision and audition quantitavely in terms of the different auditory &ldquo;patterns <em>at the
cortical level</em>&rdquo;, and takes the shortcut to compare areas of the two parts of the cortex.<br>
Vision:audition is 100:1. So recovering full audition with vision would yield still 95% vision, but
the contrary would yield 10% vision (non-linear, based on distance at which a resolution pattern is
reached).</p>
<h2 id="vii-cybernetics-and-psychopathology">VII. Cybernetics and Psychopathology</h2>
<p>p144. The brain is considered a <em>computing machine</em>, at least because this comparison/lens should
help with psychopathology and maybe psychiatry.</p>
<p>p145. <em>Method of checking</em> is discussed. A good one is to &ldquo;refer every operation simultaneously to
two or three separate mechanism&rdquo;.</p>
<p>Then, the <em>majority report</em> is accepted by the <em>collation mechanism</em>.<br>
<em>Minority reports</em> are indicated through a signal consisting of where and how they differ from
majority.</p>
<p>Lewis Caroll in <em>The Hunting of the Snark</em>: &ldquo;what I tell you three times is true&rdquo;.</p>
<p>p146. <em>Functional</em> disorders: they are not based on physiological or anatomical issues.</p>
<p>Notes again that the (adult) brain is not the empty physical structure of the computing machine
that corresponds to it, but</p>
<blockquote>
<p>the combination of this structure with the instructions given it at the beginning of a chain of
operations and with all the additional information stored and gained from outsdie in the course of
this chain.</p>
</blockquote>
<p>Difference between <em>circulating memories</em> and <em>long-term memories</em>.</p>
<p>We are pretty sure we can&rsquo;t reconstruct the ideational content which is recorded out of chains of
neurons and synapses when the brain is dead. Figuring a treshold for a synapse after death looks
difficult.</p>
<p>Alteration of synaptic tresholds could be the cause of functional disorders. Even paresis.</p>
<p>p147. Link between the <em>specious present</em> and <em>anxiety neuroses</em>.</p>
<p>Memories of the specious present normally dissipate. But some eat-up the neuron pool.<br>
This second case is a <em>malignant worry</em>.</p>
<p>This will eat-up short-term mental capacity. But, worse, permanent memory might get involved. The
&ldquo;ordinary mental life&rdquo; can thus be destroyed.</p>
<p>Some similar pathological processes happen in machines. Mentions never-ending, circular processes.<br>
Such contingencies happen due to highly improbable configurations.</p>
<p>üí° As if ending up in a Langton&rsquo;s ant&rsquo;s highway. It is an extremly highly complex mathematical
problem to figure which situations end up in highways.</p>
<p>p148. <em>Clearing</em>.</p>
<p>Solutions to this: clear the machine of all information, shake it, send large electric impulses,
disconnect an erring part of apparatus. All of this in hope that the circular process stops.</p>
<p>Most efficient but irreversible is death.</p>
<p>Closest non-pathological: sleeping. &ldquo;Sleep over it!&rdquo;.</p>
<p>Prefrontal lobotomy, in vogue at the time. Maybe more in use because easing custodial care.<br>
But this is actually working to remove a malignant worry, by &ldquo;damaging or destroying the capacity
for maintained worry&rdquo;, otherwise called <strong><em>conscience</em></strong>.<br>
Limits all access to circulating memory.</p>
<blockquote>
<p>Let me remark in passing that killing them makes their custodial care still easier.</p>
</blockquote>
<p>Less deleterious: shock treatment. Can damage memory (on purpose).</p>
<p>p149. On deeper-seated permanent memories. there seems to be no pharmaceutical or surgical weapon
against this.</p>
<p>That&rsquo;s where <em>psychoanalysis</em> and other psychotherapeutic measures come in.<br>
These techniques are based on &ldquo;the concept that the stored information of the mind lies on many
levels of acessibility an dis much richer and more varied than that which is accessible by direct
unaided introspection&rdquo;.</p>
<p>p150. Mentions limits in systems (based on D&rsquo;Arcy Thompson):</p>
<blockquote>
<p>each form of organization has an upper limit of size, beyond which it will not function.</p>
</blockquote>
<p>In telephone switching systems, the high number of stages implies that, until traffic increases to
the critical point, the system functions well, and breaks down right after.<br>
Indeed, probability of success is <code>p^1/n</code> with p probability of success at each stage. p ‚ÜòÔ∏è   =&gt;
quality ‚ÜòÔ∏è ‚ÜòÔ∏è  .</p>
<p>Same goes for Man:</p>
<blockquote>
<p>is then likely to perform a complicated type of behavior efficiently very close to the edge of an
overload, when he will give way in a serious and catastrophic way.</p>
</blockquote>
<p>Due to excess in amount of traffic, physical removal of channels , excessive occupation of channels
(like pathological worries).</p>
<p>Long neuron chains =&gt; more mental disorders.</p>
<p>Size of brain is cube of linear dimensions, but connections is square.<br>
With gyrus and sulci, the brain is optimising for short-distance, at the expense of longer-distance
communication.<br>
Thus, in a case of a traffic jam, &ldquo;the processes involving parts of the brain quite remote from one
another should suffer first&rdquo;.</p>
<blockquote>
<p>The higher processes deteriorate first in insanity.</p>
</blockquote>
<p>üí° That&rsquo;s evidence for the cybernetic view of the brain.</p>
<blockquote>
<p>The cerebral functions are not distributed evenly over the two hemispheres, and one of these, the
doinant hemisphere, as the lions' share of the higher functions.</p>
</blockquote>
<p>p153. Hemispheres</p>
<p>Damages in the non-dominant hemisphere have very small consequences on higher functions. For
example, Pasteur had his right brain basically dead, but did &ldquo;some of his best work&rdquo; after the
injury.</p>
<p>On forced educational change of handedness (for left-handers), &ldquo;in very many cases these
hemispheric changelings suffered from stuttering and other defects of speech, reading, and
writing‚Ä¶&rdquo;.<br>
One possible explanation:</p>
<blockquote>
<p>With the education of the secondary hand, there as been a partial education of that part of the
secondary hemisphere which deals with skilled motions, such as writing. Since, hoever, these
motions are carried out in the closest possible asssociation with reading, speech and other
activities which are inseparably connected with the dominant hemisphere, the neuron chains
involved in processes of the sort must cross over from hemisphere to hemisphere and back‚Ä¶</p>
</blockquote>
<p>Knowing that cerebral commissures (between hemispheres) are so few, a huge traffic jam then happens.</p>
<blockquote>
<p>the human brain is probably too large already to use in an efficient manner all the facilities
which seem to be anatomically present.</p>
</blockquote>
<blockquote>
<p>we may be facing one of those limitations of nature in which highly specialized organs reach a
level of declining efficiency and ultimately lead to the extinction of the species.</p>
</blockquote>
<h2 id="viii-information-language-and-society">VIII. Information, Language, and Society</h2>
<p>p155. Leviathan of Hobbes: Man-State made up of lesser men
=&gt;
Leibniz (monads?) living organism a plenum with smaller organisms with their own life.</p>
<p>Anticipation of the <em>cell theory</em>.</p>
<p>The community of social animals (including Man and bees), looks like an individuality.</p>
<blockquote>
<p>All the nervous tissue of the beehive is the nervous tissue of some single bee.</p>
</blockquote>
<p>p156. <em>Intercommunication</em> of the members of a society/community.</p>
<p>Hormones is an example (sexual hormones).</p>
<p>About non-verbal communication (with a savage), &ldquo;a signal without an intrinsic content may acquire
meaning in his mind by what heobserves at the time&rdquo;.</p>
<blockquote>
<p>Thus social animals may have an active, intelligent, flexible means of communication long before
the development of language.</p>
</blockquote>
<p>Distinguish <em>amount of information available to the race</em> from <em>available to the individual</em>.<br>
Racial informaiton is the one that changes an individual to act differently in a recognizable manner
by other members of the race.</p>
<p>p158. <em>Group</em> (of individuals).</p>
<p>Can be characterised by <em>autonomy</em> (measured by number of decisions inside vs outside) and
<em>effective size</em> (size to achieve some degreee of autonomy).</p>
<p>Makes a clear separation between individual and race-level information.<br>
Makes a criticism of Bush&rsquo;s Memex, since according to him we would need a human with gigantic
knowledge to perform comparisons between material (books). [Looks like bad judgement]</p>
<p>p158. <em>Homeostasis</em> and the lack of it.</p>
<p>The <em>body politic</em> laks efficient homeostatic processes.<br>
Which is similar to higher business life, politics, diplomacy and war, in which there is no
homestasis: parties look for their own interest, sometimes form coalitions but only to be soon
betrayed.</p>
<blockquote>
<p>We are involved in the business cycles of boom and failure, in the succession of dicatatorship and
revolution, in the wars which everyone loses, which are so real a feature of modern times.</p>
</blockquote>
<p>Considers these social constructs, like business, as games, which fall under <em>game theory</em> (Von
Neumann and Morgenstern).<br>
But players of real-life games are are not totally rational players as in game theory.</p>
<p>Real-life games, &ldquo;with the common man as their object&rdquo;, are making him a loser (tricking him into
voting, buying‚Ä¶).</p>
<p>But, for the author, closely knit communities that have been there for long enough are able to reach
a good level of care and management of the issues of society: the &ldquo;have a considerable measure of
homeostasis&rdquo;.</p>
<p>üí° This is probably not true in a rapidly changing world &amp; society, is it?</p>
<p>Also, the author points out that he most effective and important <em>anti-homeostasis factor</em> of
society is when <em>&ldquo;Lords of Things as They Are&rdquo;</em> (who want to play the game and gain power) control
<strong>means of communication</strong> (notably: reduce private criticism, etc.).</p>
<blockquote>
<p>One of the lessons of the present book is that any organism is held together in this action by the
possession of means for the acquisition, use, retention and transmission of information.</p>
</blockquote>
<p>p161. Power as a threat for homeostasis in society.</p>
<p>Notably, pervasiveness of business in media.</p>
<blockquote>
<p>we have a triple constriction of the means of communication: the elimination of the less
profitable means in favor of the more profitable; the fact that these means are in the hands of
the very limited class of wealthy men, and thus naturally express the opinions of that class; and
the further fact that, as one of the chief avenues to political and personal power, they attract
above all those ambitious for such power.</p>
</blockquote>
<p>Makes the point that the larger community is sometimes more stupid than the individual. And
sometimes is less intelligent than the smaller community, notably in terms of proper sharing of
information because of power agents (see above).</p>
<p>The author is pretty pessimistic that &ldquo;anarchy of modern society&rdquo; can be resolved.</p>
<p>p162. <strong>Extending <em>methods of the natural sciences</em> to social sciences is not the solution.</strong></p>
<p>People believing this show &ldquo;excessive optimism, and a misunderstanding of the nature of all
scientific achievement&rdquo;: <strong>we can&rsquo;t isolate well social phenomena</strong>.</p>
<p><em>Loose coupling</em> is well achieved with phenomena involving stars or molecules (maybe &ldquo;mass effects&rdquo;
for the last ones, but not significant).</p>
<blockquote>
<p>It is in the social siences that the coupling between the observed phenomenon and the observer is
hardest to minimize.</p>
</blockquote>
<p><strong><em>Traduttore, traditore</em></strong>: there is impact on and reduction of who you study.</p>
<blockquote>
<p>the social scientist has not the advantage of looking down on his subjects from the cold heights
of eternity and ubiquity.</p>
</blockquote>
<p>üí° Basically saying that real <em>self-objectification</em> is impossible.</p>
<p>Gives the example that social scientists and philosophers are always reducing their field of study
to what relates to them: people around him, durations limited to a lifespan, ideas of the moment.</p>
<blockquote>
<p>in the social sciences we have to deal with short statistical runs,
nor can we be sure that a considerable part of what we observe is not an artifact of our own
creation.
We are too much in tune with the objects of our investigation to be good probes.</p>
</blockquote>
<p>Too small data compared to hard sciences.</p>
<blockquote>
<p>There is much which we must leave, whether we like it or not, to the un-&ldquo;scientific,&rdquo; narrative
method of the professional historian.</p>
</blockquote>
<p>üí° Ask an AI to study us?</p>
<h2 id="ix-on-learning-and-self-reproducing-machines">IX. On Learning and Self-Reproducing Machines</h2>
<p>p169. Assertion that power to learn (<em>ontogenetic learning</em>) is similar to power to reproduce
(<em>phylogenetic learning</em>) in living systems.</p>
<p>p170. What about man-made machines? Yes, they can learn and reproduce.</p>
<p>p170. About <em>playing games</em>.</p>
<p>There is the Von Neumann theory: how to build a &ldquo;complete strategy&rdquo;, working state by state
backwards from the end (winning) move.<br>
Works only for very simple games (as complexity explodes).</p>
<p>There is also the statistical approach: look at past actions.<br>
Works well with strategic games like war, checkers and chess.</p>
<p>A machine could work with that on chess for example, by examining previous games recorded.<br>
That might need structuring the learning process in several steps.
üí° This is basically a preview of machine learning engineering.</p>
<p>p173. About linearity of feedback.</p>
<p>Mentions <em>first-order programming</em> (linear feedback) and <em>second-order programming</em> (more extensive
use of the past and non-linear feedback).</p>
<p>üî¥ Are the terms really used in the previous chapters?</p>
<p>Mentions Watanabe <em>Information Theoretical Analysis of Multivariate Correlation</em> (1960) which is
about finding the most elegant solution to a geometrical problem.</p>
<p>p174. <em>Theory of game-playing machines</em>, applied to the &ldquo;activity of <em>struggle</em>&rdquo;.</p>
<p>üí° Again an instance of (swiftly) applying a theory as a prism to understand and manipulate the
world. In some way, the &ldquo;way of the engineer&rdquo;?</p>
<p>Mentions how it can help analysing mongoose vs cobra: the cobra only does single actions one after
the other (kind of a first-order feedback machine?) whereas the mongoose&rsquo;s action &ldquo;involves an
appreciable, if not very long, segment f the whole past of the fight&rdquo; (second-order feedback? at
least we see clearly that there is a lower-frequency feedback).</p>
<blockquote>
<p>&hellip;it must be remembered that the bullfight is not a sport but a dance with death, to exhibit the
beauty and the interlaced coordinatng actions of the bull and the man.</p>
</blockquote>
<p>p175. On the use of learning machines (<em>mechanization</em>) in real games, notably war.</p>
<p>Asserts that WWIII biggest danger maybe comes from the &ldquo;unguarded use of learning
machines&rdquo;. Notably, the problem of not being able to turn them off:</p>
<blockquote>
<p>To turn a machine off effectively, we must be in possession of information as to whether the
danger point has come.</p>
</blockquote>
<p>But the checker-playing machine can defeat its programmer.<br>
üí° Maybe not a strong argument. Rather see Hamming lectures.</p>
<p>p176. Discusses the history of the dangers given to machine, in older tales in the form of the
problem of magic, talking about &ldquo;the moral situation of the magician&rdquo;.</p>
<p>Mentions Goethe&rsquo;s &ldquo;The Sorcerer&rsquo;s Apprentice&rdquo;, Arabian Nights' genie and W. W. Jacobs' fable of the
monkey&rsquo;s paw.</p>
<blockquote>
<p>In all these stories the point is that the agencies of magic are literal-minded¬†; and that if we
ask for a boon from them ,we must ask for what we really want and not for what we think we want.
The new and real agencies of the learning machine are also literal-minded.</p>
</blockquote>
<blockquote>
<p>We cannot expect the machine to follow us in thos prejudices and emotional compromises by which
we enable ourselves to call destruction by the name of victory. If we ask for victory and do not
know what we mean by it, we shall find the ghost knowing at our door.</p>
</blockquote>
<p>p177. <em>Self-propagating machines</em>: <strong>creation of a replica capable of the same functions.</strong></p>
<p>Is this possible combinatorially-speaking? Can we build a machine with complex-enough to do that?
Yes according to J. von Neumann.</p>
<p>What about the operating procedure for building them? Introduces the <em>non-linear transducer</em> which
is a very general approach.</p>
<p>Notes that <em>non-linear transducers</em> can be built with a linear combination of smaller non-linear
parts, which allows for some learning techniques to apply (least-squares for example).  And more, we
the author shows that &ldquo;we can imitate any unknown non-linear transducer by a sum of linar terms&rdquo;,
thanks to the ergodic property. Devices exist for all of the operations needed to produce such
apparatus.</p>
<p>To determine the coefficients for the linear parts, we can use feedback apparatus.</p>
<blockquote>
<p>What we have succeeded in doing is to make a white box which can potentially assume the
characteristics of any non-linear transducer whatever, and then to draw it into the similitude of
a given black-box transducer (&hellip;) without any intervention on our part.</p>
</blockquote>
<p>Compares this &ldquo;philosophically&rdquo; to the way genes are able to produce other molecules of the same
gene with at their disposal &ldquo;an indeterminate mixture of amino and nucleic acids&rdquo;. Or viruses.</p>
<h2 id="x-brain-waves-and-self-organizing-systems">X. Brain Waves and Self-Organizing Systems</h2>
<p>p181. This chapter is about a specific self-organizing system where non-linear parts are central:
<strong>the self-organization of <em>electroencephalograms</em> or <em>brain waves</em></strong>.</p>
<p>p181. Historical overview of <em>electrophysiology</em> which is about electrical potentials in nervous
systems.</p>
<p>This science was slow as start because instruments were &ldquo;not good enough to record small electrical
potentials without heavy distortions&rdquo;.</p>
<p>The new technique to accelerate this has been <em>electronics</em>: conduction of gases (vacuum tube) by
Edison then conduction <em>in vacuo</em> (cathode-ray oscillograph).</p>
<p>Allow to follow the time-course of small potentials, between electrodes on the scalp.</p>
<p>p183. But the mathematical understanding of oscillation has been troublesome. At first, only the
alpha rhythm were clearly detected (1/10th of a second).</p>
<p>The author has been then working on this (around 1930), using <em>autocorrelation</em>: time mean of
<code>∆í(t + œÑ)∆í(t)</code> (or conjugate multiplication).</p>
<p>For that:</p>
<ul>
<li>To record better, the use of magnetic tape with frequency modulation is necessary.</li>
<li>Delayed tape reading, using distance between playback heads: <code>f(t)</code> and <code>f(t + œÑ)</code></li>
<li>Multiply the two using square-law rectifiers and linear mixers based on <code>4ab=(a+b)¬≤-(a-b)¬≤</code>.</li>
<li>Approximate averageing using resistor-capacitor network with time constant long compared with the
duration of the sample.</li>
<li>Repeat the process for multiple œÑ.</li>
</ul>
<p>üåÖ Figure 9 represents autocorrelation measured for œÑ varying from 0 to 17 seconds. It shows an
oscillation of wave between 0.1 and 0.2 seconds, with amplitude reducing when œÑ ‚ÜóÔ∏è .</p>
<p>Makes the point that this is similar to Michelson&rsquo;s interferometer in which the intensity of the
interferometer fringes gives autocorrelation (&ldquo;except for a linear transformation&rdquo;).</p>
<p>üí° We can allow ourselves to generally <strong>think of data as &ldquo;except for a linear transformation&rdquo;.</strong><br>
Indeed, linear apparatus is both easy to abstract-out mathematically but also in practical means
because it is easy to build and commonly found in nature.<br>
Thus, rather than manipulating data (for example the amplitude given by some interference fringes or
by Wiener&rsquo;s oscillator) we can directly manipulate its equivalence class of any other amount
calculable with a linear transformation (or even, maybe, multiple of them).<br>
For the question of determining the coefficients of the linear transformation, if they are unknown,
simple feedback mechanism exist as mentioned above.</p>
<p>p186. How to obtain a <em>spectrum of a brain wave</em> from autocorrelation.</p>
<p>Writes the autocorrelation in a Fourier form: <code>C(t) = ‚à´ exp(2œÄi‚çµt) dF(‚çµ)</code>, <code>F</code> ‚ÜóÔ∏è  called <em>integrated
spectrum of ∆í</em>.</p>
<p>Putting aside some specific part of the spectrum, we have <code>C(t) = ‚à´ exp(2œÄi‚çµt) œï(‚çµ) d‚çµ</code>, œï the
<em>spectral density</em>. If œï is L‚ÇÇ, we have <code>œï(‚çµ) = ‚à´ C(t)exp(-2œÄi‚çµt) dt</code> which will look like a peak
around -10 and one other around 10, 0 otherwise (&ldquo;10 cycles&rdquo;).</p>
<p><em>Heterodyning method</em>: using some basic transforms (shifting frequency distributions to have peaks
around 0, getting the spectrum to the right and to the left of the central frequency at the distance
‚çµ‚Ä¶), we can build the spectrum.</p>
<p>The method is simpler for autocorrelations which are nearly sinusoidal. For that, we can take
autocorrelations at regular intervals (0, 1/n, 2/n‚Ä¶), and average them in a certain way to remove
the cosine component and keep only the sine. We only need to multiply with 1 or -1, which is easily
practicable even with manual means.</p>
<p>Computers have started to be used, so that heterodyning is less needed.</p>
<p>p190. Results of harmonic analysis of brain waves.</p>
<p>üåÖ Fig 11. Resulsts of the harmonic analysis of the autocorrelation function of Fig 9.<br>
Peak around 8.9 then dip then 9.0, then drop at 9.05.</p>
<blockquote>
<p>There is a strong suggestiong that the power in the peak corresponds to a pulling of the power
away from the region where the curve is low.</p>
</blockquote>
<p>After another electroencephalogram of the <strong>same subject</strong> four days later, the approx <strong>width of
the peak is retained</strong>, as well as the form.</p>
<p>p192. <em>Sampling problem</em>, using integration in function space.</p>
<blockquote>
<p>we shall be able to construct a statistical model of a continuing process with a given spectrum.</p>
</blockquote>
<p>Such a model is enough to &ldquo;yield statistically significant information concerning the
root-mean-square error to be expected in bran-wave spectra&rdquo;.</p>
<p>üí° All that matters for scientific investigation of a given object/process is to have a good enough
approximation. For that, we use statistical models and consider their root-mean-square error.</p>
<p><code>x(t,‚ç∫)</code> ‚Ñù, [0, 1] -&gt; ‚Ñù, representing one space variable of a Brownian motion. ‚ç∫ a statistical
distribution.</p>
<p>We look at <code>‚à´œï(t)dx(t,‚ç∫)</code>.</p>
<p>If <code>‚Ñ±</code> functional with <code>‚Ñ±[x(t,‚ç∫)]</code> function of ‚ç∫ depending only on differences <code>x(t‚ÇÇ,‚ç∫) - x(t‚ÇÅ,‚ç∫)</code>
(meaning it depends only on the evolution of x in time), Birkhoff&rsquo;s ergodic theorem can be applied.</p>
<p>Examines the response of a linear transducer to a Brownian motion <code>∆í(t,‚ç∫) = ‚à´K(t+œÑ)dx(œÑ,‚ç∫)</code>.<br>
Its autocorrelation (integral of <code>∆í(t+œÑ,‚ç∫)∆í*(t,‚ç∫)</code>) can have the ergodic theorem applied.</p>
<p>Calculates the spectrum and the samped spectrum where the autocorrelation is sampled over an
averaging time: &ldquo;the sampled spectrum and the true spectrum will have the same time-average value&rdquo;.</p>
<p>Looks at the approximate spectrum, where integration of œÑ is done over [0, 20 seconds].<br>
Then calculated the root-mean-square error of the approximate sampled spectrum, and ends up with
1/6.</p>
<p>p197. Physilogocial questions about the dip phenomenon in frequency.</p>
<blockquote>
<p>a sharp frequency line is equivalent to an accurate clock.</p>
</blockquote>
<p>Notes that other forms of <strong><em>control and computation apparatus</em></strong>, other than the brain, use clocks
for the purpose of <em>gating</em>: combine a large number of impulses into single impulses.</p>
<p>For desirable functioning of the apparartus, messages must be stored then released simultaneously,
and combined while they are still on the machine. Thus gating and clock.</p>
<p><em>Synaptic mechanism</em>: numerous fibers incoming, one outgoing.<br>
For example, the outgoing fiber fire
only when the proper combination of incoming ones fire in a very short interval of time.<br>
In all cases, <strong>a short combination period is essential</strong> to combine incoming messages.</p>
<p>p198. The central <em>alpha rhythm</em> of the brain.</p>
<p><strong>There is experimental evidence of gating in the nervous system.</strong><br>
It is well known that there is a delay bewteen an incoming visual signal and consequent muscular
activity.<br>
The delay has been shown to be consisting of 3 parts: one constant, 2 others uniformly distributed
around 0.1s.</p>
<blockquote>
<p>It is as if the central nervous system could pick up incoming impulses only every 1/10 second, and
as if the outgoing impulses to the muscales could arrive from the central nervous system only
every 1/10 second.</p>
</blockquote>
<p>üí° Again here the author uses an electric-apparatus model to describe the functioning of the brain.
This is done in a &ldquo;successful&rdquo; or positive way, in the sense that it allows us next to build upon
abstractions developed in the context of electricity theory and related technical apparatus.</p>
<p>The alpha rhythm can modified by several means, light or electrostatic induction notably:</p>
<blockquote>
<p>If a light is flickered into the eye at intervals with a period near 1/10 second, the alpha rhythm
of the brain is modified until it has a strong component of the same period as the flicker.</p>
</blockquote>
<p>p199. Analysis based on oscialltors and linear/non-linear mechanisms.</p>
<blockquote>
<p>It is important to observe that if the frequency of an oscillator can be changed by impulses of a
different frequency, the mechanism must be non-linear.</p>
</blockquote>
<p>Indeed linear only changes phase and amplitude.</p>
<p>A non-linear mechanism can displace frequency, usually by <strong>attraction</strong>.<br>
Considers that the attraction is probably a long-time phenomenon, whereas the system behaves
short-time like approximately linearly.</p>
<p>Models the brain as &ldquo;a number of osciallators of frequencies of nearly 10 per second&rdquo; and &ldquo;within
limitations these frequencies can be attracted to one another&rdquo;.<br>
As a consequence, frequencies are likely to be pulled together in certain regions of the spectrum,
thus causing gaps in the spectrum. This corresponds closely to what is observed in the spectral
analysis.</p>
<p>p200. Examines the brain for the existence and nature of the osciallators postulated.</p>
<p>It has been ovserved that the cerebral cortex potentials osciallate with a 1/10s frequency (before
dying out) after a flash of light is delivered to the eyes.</p>
<p>p200. <em>Pulling together of frequencies</em>: evidence in nature, correspondance with the previous model.</p>
<p>üí° Once you have a model, apply it everywhere! Lenses. But powerful.</p>
<p>Applies the same thinking to show that there is also <strong>pulling together of short-term osciallations
in a continuing oscillation</strong> in case of the diurnal rhythm of living beings, attracted to the
24-hour rhythm of the external environment.</p>
<p>Same goes with fireflies flahses, which are synchronized.<br>
Should be modifyable by a flashing neon tube.</p>
<p>The phenomenon exists also in non-living beings.<br>
Mentions electrical generating systems with parallel busbars which show a frequency regulation
behavior.<br>
But in series, this would show repulsion and thus be unstable, as previously observed.</p>
<blockquote>
<p>The parallel system had a better homeoststis than the series system and therefore survived, while
the series system eliminated itself by natural selection.</p>
</blockquote>
<p>üí° Lenses in lenses, models in models.</p>
<p>p202. Relationship with the living and the brain. Generalization.</p>
<blockquote>
<p>We thus see that a non-linear interaction causing the attraction of frequency can generate a
self-organizing system</p>
</blockquote>
<p>Mentions the problem of molecules in the mixture of amino and nucelic acids (&quot;<em>indifferent magma</em>&quot;) in
cells can produce definite behavior, which is unclear.<br>
This is at the basis of the &ldquo;fundamental phenomenon of life&rdquo;: reproducing macromolecules.<br>
Suggests that &ldquo;the active bearer of the specificity of a modecule may lie in the frequency parttern
of its molecular radiation&rdquo;.<br>
Thus, for example, a virus could emit infra-red osciallations which in turn favor the formation of
other molecules of virues from the indifferent magma.</p>
<p>Recognises that it&rsquo;s speculative. But deserves to be invesetigated, and presents a method for this:
&ldquo;study the absorption and emission spectra of a massive quantity of virus material&rdquo;.</p>
<p>üí° Generalization of applicability of model. Speculative. Innovative. Creates directions for future
research programs.</p>
<h1 id="compte-rendu-g√©n√©ral">Compte-rendu g√©n√©ral</h1>
<p>Les id√©es que j&rsquo;en ressorts:</p>
<ul>
<li>En termes de synth√®se globale du contenu du livre, et l&rsquo;une des th√®ses principale de l&rsquo;auteur: les
sciences et techniques de la 2e moiti√© du 20e si√®cle vont √™tre domin√©es par la communication et le
contr√¥le. Domination? Recherches et mises en pratique, mais aussi dans le sens o√π ces techniques
vont √™tre en ma√Ætrise des techniques pr√©c√©dentes (m√©caniques, √©lectriques, ‚Ä¶). On peut ainsi
pr√©voir un shift de pouvoir dans la soci√©t√©?</li>
<li>Synth√®se des ouvertures que propose l&rsquo;auteur sur le rapport avec l&rsquo;humain, et autre th√®se
fondamentale de l&rsquo;auteur: le cerveau (et corps) humain sera mieux (si ce n&rsquo;est int√©gralement?)
compris par l&rsquo;usage des sciences cybern√©tiques, et ainsi nous verrons un progr√®s m√©dical certain.
Cette compr√©hension et les applications &ldquo;techniques&rdquo; qui s&rsquo;ensuiveront (appareils, m√©dicaments,
actes m√©dicaux / psychiatriques, etc.) sont directement issues de sciences et techniques
appliqu√©es aux machines. En comprenant la machine, l&rsquo;humain scientifique d√©veloppe une
compr√©hension plus valide (mais aussi plus imm√©diate?) de sa propre science, compr√©hension qu&rsquo;il
pourra par analogie appliquer √† l&rsquo;humain lui-m√™me.</li>
<li>Le raisonnement par analogie est pr√©pond√©rant pour faire justement le lien entre compr√©hension
de / application √† (disons &ldquo;rapport scientifico-technique √†&rdquo;, ou simplement &ldquo;rapport √†&rdquo;) la
machine et rapport au vivant, en particulier l&rsquo;humain. Mais aussi rapport √† la soci√©t√© (sciences
sociales). Ce raisonnement par analogie est d√©fendu comme une pierre angulaire du bon fondement du
raisonnement scientifique moderne (peut-√™tre industrialiste?) qui vise un progr√®s via
l&rsquo;application la plus syt√©matique possible (‚ùå) des sciences.</li>
<li>Sur le plan plus personnel, ce type de lecture semble vraiment essentielle pour construire un
esprit scientifique &ldquo;appliqu√©&rdquo;, et compl√©menter ma formation d&rsquo;ing√©nieur. Je comprends maintenant
non seulement mieux certains r√©sultats physiques valid√©s et l&rsquo;approche globale de leur
d√©monstration, et leurs liens dans l&rsquo;histoire des sciences (et de la pens√©e).</li>
</ul>


  </article>


      <footer class="site-footer">
        <span itemscope itemtype="http://schema.org/Person">
          <link itemprop="url" href="https://www.16h30s.com/">
          <span itemprop="name">Alexandre Hajjar</span>

          <br>

          

          

          
        </span>

        
        <br>¬© 2021
      </footer>
    </div>

  <script src="/js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  </body>
</html>

